# ğŸš€ Ejecutar un LLM Local con Ollama y TinyLlama

Esta guÃ­a te permitirÃ¡ ejecutar un modelo de lenguaje local usando Ollama y TinyLlama en tu entorno de desarrollo.

## ğŸ“‹ Â¿QuÃ© es Ollama?

**Ollama** es una plataforma de cÃ³digo abierto que permite ejecutar modelos de lenguaje de gran tamaÃ±o localmente en tu mÃ¡quina. Ofrece:

- ğŸ”’ **Privacidad total**: Tus datos nunca salen de tu mÃ¡quina
- âš¡ **Rendimiento**: EjecuciÃ³n local sin latencia de red
- ğŸ› ï¸ **FÃ¡cil instalaciÃ³n**: Un solo comando para instalar
- ğŸ¯ **MÃºltiples modelos**: Soporte para diversos modelos de lenguaje

## ğŸ§  Â¿QuÃ© es TinyLlama?

**TinyLlama** es un modelo de lenguaje compacto con **1.1 mil millones de parÃ¡metros**, diseÃ±ado para ser:

- ğŸ“¦ **Ligero**: Solo ~1GB de tamaÃ±o
- âš¡ **RÃ¡pido**: Optimizado para entornos con recursos limitados
- ğŸ¯ **Eficiente**: Ideal para desarrollo y pruebas
- ğŸ”§ **Perfecto para Codespaces**: Funciona bien con 4GB de RAM

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1ï¸âƒ£ Instalar Ollama

En tu terminal (Codespace):

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Â¿QuÃ© hace este comando?**
- Descarga el script de instalaciÃ³n oficial de Ollama
- Instala Ollama en tu sistema
- Configura las dependencias necesarias

### 2ï¸âƒ£ Iniciar el Servicio

```bash
ollama serve &
```

**Â¿QuÃ© hace este comando?**
- Inicia el servicio de Ollama en segundo plano
- El `&` permite que el servicio corra en background
- Habilita la comunicaciÃ³n con los modelos

### 3ï¸âƒ£ Descargar TinyLlama

```bash
ollama pull tinyllama
```

**Â¿QuÃ© hace este comando?**
- Descarga el modelo TinyLlama-1.1B (~1GB)
- Lo almacena localmente en tu sistema
- Lo prepara para su uso

### 4ï¸âƒ£ Ejecutar el Modelo

```bash
ollama run tinyllama
```

**Â¿QuÃ© hace este comando?**
- Inicia una sesiÃ³n interactiva con TinyLlama
- Te permite escribir prompts directamente
- El modelo responde en tiempo real

## ğŸ’¬ Ejemplo de Uso

Una vez ejecutado `ollama run tinyllama`, puedes interactuar asÃ­:

```
>>> Â¿QuÃ© es un modelo de lenguaje pequeÃ±o?

...
```

## ğŸ”§ Comandos Ãštiles

```bash
# Ver modelos instalados
ollama list

# Detener un modelo
ollama stop tinyllama

# Ver informaciÃ³n del modelo
ollama show tinyllama

# Eliminar un modelo
ollama rm tinyllama
```

## ğŸ¯ Ventajas de Usar LLMs Locales

- **ğŸ”’ Privacidad**: Tus datos nunca salen de tu mÃ¡quina
- **âš¡ Velocidad**: Sin latencia de red
- **ğŸ’° Costo**: Sin costos por API
- **ğŸ› ï¸ Control**: ConfiguraciÃ³n completa del entorno
- **ğŸ“š Aprendizaje**: Entender cÃ³mo funcionan los LLMs

## ğŸš¨ Consideraciones

- **Memoria**: TinyLlama requiere ~2GB de RAM
- **Almacenamiento**: El modelo ocupa ~1GB de espacio
- **Rendimiento**: MÃ¡s lento que modelos en la nube, pero mÃ¡s privado

## ğŸ”— Recursos Adicionales

- [Ollama Official Website](https://ollama.com)
- [TinyLlama Paper](https://arxiv.org/abs/2401.02385)
- [Ollama GitHub Repository](https://github.com/ollama/ollama)

---

**Â¡Listo!** ğŸ‰ Ahora tienes tu propio modelo de lenguaje ejecutÃ¡ndose localmente.
